# -*- coding: utf-8 -*-
"""Model Assessment - Engineered Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s5sRkQnFYrK5sd4b0HHLCLzDvnylD_Uu

#
---
# Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
import torch
# %load_ext Cython
import torch.nn as nn
import torch.optim as optim
import seaborn as sns
import matplotlib.pyplot as plt
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, fbeta_score, precision_score, recall_score, average_precision_score

"""#
---
# Importing Dataset
"""

# data.npz: feature engineered dataset
data = np.load('data.npz')
trainx = data['arr1']
trainy = data['arr2']
testx = data['arr3']
testy = data['arr4']

df_log

"""#
---
# Machine Learning Models

#### ◉ Logistic Regression (Python)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Define the Logistic Regression model
# model = LogisticRegression(solver='saga', max_iter=100000)
# 
# # Define the parameter grid for Grid Search
# param_grid = {
#     'penalty': ['l1', 'l2'],
#     'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]
# }
# 
# # Create Grid Search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
# # Fit the model to find the best parameters
# grid_search.fit(trainx, trainy)
# 
# # Get the best parameters
# best_params = grid_search.best_params_
# print("Best Parameters:", best_params)
# 
# # Use the best parameters to create the final model
# best_model = LogisticRegression(**best_params, solver='saga', max_iter=100000)
# best_model.fit(trainx, trainy)
# 
# # Make predictions on the training set
# trainy_pred = best_model.predict(trainx)
# 
# # Evaluate the model on the training set
# accuracy_train = accuracy_score(trainy, trainy_pred)
# print("Training Accuracy:", accuracy_train)
# 
# # Make predictions on the test set
# testy_pred = best_model.predict(testx)
# 
# # Evaluate the model on the test set
# accuracy = accuracy_score(testy, testy_pred)
# report = classification_report(testy, testy_pred, digits=5)
# auc = roc_auc_score(testy, testy_pred)
# f2 = fbeta_score(testy, testy_pred, beta=2)
# precision_per_class = precision_score(testy, testy_pred, average=None)
# f1 = fbeta_score(testy, testy_pred, beta=1)
# average_precision = average_precision_score(testy, testy_pred)
# weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
# # Additional Metrics
# print("Accuracy:", accuracy)
# print("AUC:", auc)
# print("F1-score:", f1)
# print("F2-score:", f2)
# print("Precision per class:", precision_per_class)
# print("Average Precision:", average_precision)
# print("Weighted Average Recall:", weighted_recall)
# print("Classification Report:\n", report)
# 
# # Confusion Matrix Visualization
# plt.rcParams['figure.figsize'] = (4, 4)
# sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
# plt.title('Confusion Matrix')
# plt.show()
# 
# # True Positive, True Negative, False Positive, False Negative
# tp = confusion_matrix(testy, testy_pred)[1][1]
# tn = confusion_matrix(testy, testy_pred)[0][0]
# fp = confusion_matrix(testy, testy_pred)[0][1]
# fn = confusion_matrix(testy, testy_pred)[1][0]
# 
# print('True Positive Cases: {}'.format(tp))
# print('True Negative Cases: {}'.format(tn))
# print('False Positive Cases: {}'.format(fp))
# print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ Logistic Regression (Cython + Parallelized GridSearchCV Process)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%cython -a
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, fbeta_score, precision_score, recall_score, average_precision_score, confusion_matrix
# import matplotlib.pyplot as plt
# from joblib import parallel_backend
# import seaborn as sns
# cimport numpy as np
# cimport cython
# 
# @cython.boundscheck(False)
# @cython.wraparound(False)
# 
# def logistic_regression_cython(np.ndarray[np.float64_t, ndim=2] trainx, np.ndarray[np.int64_t, ndim=1] trainy, np.ndarray[np.float64_t, ndim=2] testx, np.ndarray[np.int64_t, ndim=1] testy):
#     # Define the Logistic Regression model
#     model = LogisticRegression(solver='saga', max_iter=100000)
# 
#     # Define the parameter grid for Grid Search
#     param_grid = {
#         'penalty': ['l1', 'l2'],
#         'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]
#     }
# 
#     with parallel_backend('multiprocessing', n_jobs=-1):
#         grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
#         # Fit the model to find the best parameters
#         grid_search.fit(trainx, trainy)
# 
#     # Get the best parameters
#     best_params = grid_search.best_params_
#     print("Best Parameters:", best_params)
# 
#     # Use the best parameters to create the final model
#     best_model = LogisticRegression(**best_params, solver='saga', max_iter=100000)
#     best_model.fit(trainx, trainy)
# 
#     # Make predictions on the training set
#     trainy_pred = best_model.predict(trainx)
# 
#     # Evaluate the model on the training set
#     accuracy_train = accuracy_score(trainy, trainy_pred)
#     print("Training Accuracy:", accuracy_train)
# 
#     # Make predictions on the test set
#     testy_pred = best_model.predict(testx)
# 
#     # Evaluate the model on the test set
#     accuracy = accuracy_score(testy, testy_pred)
#     report = classification_report(testy, testy_pred, digits=5)
#     auc = roc_auc_score(testy, testy_pred)
#     f2 = fbeta_score(testy, testy_pred, beta=2)
#     precision_per_class = precision_score(testy, testy_pred, average=None)
#     f1 = fbeta_score(testy, testy_pred, beta=1)
#     average_precision = average_precision_score(testy, testy_pred)
#     weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
#     # Additional Metrics
#     print("Accuracy:", accuracy)
#     print("AUC:", auc)
#     print("F1-score:", f1)
#     print("F2-score:", f2)
#     print("Precision per class:", precision_per_class)
#     print("Average Precision:", average_precision)
#     print("Weighted Average Recall:", weighted_recall)
#     print("Classification Report:\n", report)
# 
#     # Confusion Matrix Visualization
#     plt.rcParams['figure.figsize'] = (4, 4)
#     sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
#     plt.title('Confusion Matrix')
#     plt.show()
# 
#     # True Positive, True Negative, False Positive, False Negative
#     tp = confusion_matrix(testy, testy_pred)[1][1]
#     tn = confusion_matrix(testy, testy_pred)[0][0]
#     fp = confusion_matrix(testy, testy_pred)[0][1]
#     fn = confusion_matrix(testy, testy_pred)[1][0]
# 
#     print('True Positive Cases: {}'.format(tp))
#     print('True Negative Cases: {}'.format(tn))
#     print('False Positive Cases: {}'.format(fp))
#     print('False Negative Cases: {}'.format(fn))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# logistic_regression_cython(trainx, trainy, testx, testy)

"""---
#### ◉ k-Nearst Neighbors (k-NN) (Python)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Define the K-NN model
# knn_model = KNeighborsClassifier()
# 
# # Define the parameter grid for Grid Search
# param_grid = {
#     'n_neighbors': [3, 11, 19, 27, 35],
#     'weights': ['uniform', 'distance'],
#     'p': [1, 2]
# }
# 
# # Create Grid Search with cross-validation
# grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
# # Fit the model to find the best parameters
# grid_search.fit(trainx, trainy)
# 
# # Get the best parameters
# best_params = grid_search.best_params_
# print("Best Parameters:", best_params)
# 
# # Use the best parameters to create the final model
# best_knn_model = KNeighborsClassifier(**best_params)
# best_knn_model.fit(trainx, trainy)
# 
# # Make predictions on the training set
# trainy_pred = best_knn_model.predict(trainx)
# 
# # Evaluate the model on the training set
# accuracy_train = accuracy_score(trainy, trainy_pred)
# print("Training Accuracy:", accuracy_train)
# 
# # Make predictions on the test set
# testy_pred = best_knn_model.predict(testx)
# 
# # Evaluate the model on the test set
# accuracy = accuracy_score(testy, testy_pred)
# report = classification_report(testy, testy_pred, digits=5)
# auc = roc_auc_score(testy, testy_pred)
# f2 = fbeta_score(testy, testy_pred, beta=2)
# precision_per_class = precision_score(testy, testy_pred, average=None)
# f1 = fbeta_score(testy, testy_pred, beta=1)
# average_precision = average_precision_score(testy, testy_pred)
# weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
# # Additional Metrics
# print("Accuracy:", accuracy)
# print("AUC:", auc)
# print("F1-score:", f1)
# print("F2-score:", f2)
# print("Precision per class:", precision_per_class)
# print("Average Precision:", average_precision)
# print("Weighted Average Recall:", weighted_recall)
# print("Classification Report:\n", report)
# 
# # Confusion Matrix Visualization
# plt.rcParams['figure.figsize'] = (4, 4)
# sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
# plt.title('Confusion Matrix')
# plt.show()
# 
# # True Positive, True Negative, False Positive, False Negative
# tp = confusion_matrix(testy, testy_pred)[1][1]
# tn = confusion_matrix(testy, testy_pred)[0][0]
# fp = confusion_matrix(testy, testy_pred)[0][1]
# fn = confusion_matrix(testy, testy_pred)[1][0]
# 
# print('True Positive Cases: {}'.format(tp))
# print('True Negative Cases: {}'.format(tn))
# print('False Positive Cases: {}'.format(fp))
# print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ k-Nearst Neighbors (k-NN) (Cython + Parallelized GridSearchCV Process)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%cython -a
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, fbeta_score, precision_score, recall_score, average_precision_score, confusion_matrix
# import matplotlib.pyplot as plt
# from joblib import parallel_backend
# import seaborn as sns
# cimport numpy as np
# cimport cython
# 
# @cython.boundscheck(False)
# @cython.wraparound(False)
# 
# cpdef knn_cython(np.ndarray[np.float64_t, ndim=2] trainx, np.ndarray[np.int64_t, ndim=1] trainy, np.ndarray[np.float64_t, ndim=2] testx, np.ndarray[np.int64_t, ndim=1] testy):
#     # Define the K-NN model
#     knn_model = KNeighborsClassifier()
# 
#     # Define the parameter grid for Grid Search
#     param_grid = {
#         'n_neighbors': [3, 11, 19, 27, 35],
#         'weights': ['uniform', 'distance'],
#         'p': [1, 2]
#     }
# 
#     with parallel_backend('multiprocessing', n_jobs=-1):
#         # Create Grid Search with cross-validation
#         grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
#         # Fit the model to find the best parameters
#         grid_search.fit(trainx, trainy)
# 
#     # Get the best parameters
#     best_params = grid_search.best_params_
#     print("Best Parameters:", best_params)
# 
#     # Use the best parameters to create the final model
#     best_knn_model = KNeighborsClassifier(**best_params)
#     best_knn_model.fit(trainx, trainy)
# 
#     # Make predictions on the training set
#     trainy_pred = best_knn_model.predict(trainx)
# 
#     # Evaluate the model on the training set
#     accuracy_train = accuracy_score(trainy, trainy_pred)
#     print("Training Accuracy:", accuracy_train)
# 
#     # Make predictions on the test set
#     testy_pred = best_knn_model.predict(testx)
# 
#     # Evaluate the model on the test set
#     accuracy = accuracy_score(testy, testy_pred)
#     report = classification_report(testy, testy_pred, digits=5)
#     auc = roc_auc_score(testy, testy_pred)
#     f2 = fbeta_score(testy, testy_pred, beta=2)
#     precision_per_class = precision_score(testy, testy_pred, average=None)
#     f1 = fbeta_score(testy, testy_pred, beta=1)
#     average_precision = average_precision_score(testy, testy_pred)
#     weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
#     # Additional Metrics
#     print("Accuracy:", accuracy)
#     print("AUC:", auc)
#     print("F1-score:", f1)
#     print("F2-score:", f2)
#     print("Precision per class:", precision_per_class)
#     print("Average Precision:", average_precision)
#     print("Weighted Average Recall:", weighted_recall)
#     print("Classification Report:\n", report)
# 
#     # Confusion Matrix Visualization
#     plt.rcParams['figure.figsize'] = (4, 4)
#     sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
#     plt.title('Confusion Matrix')
#     plt.show()
# 
#     # True Positive, True Negative, False Positive, False Negative
#     tp = confusion_matrix(testy, testy_pred)[1][1]
#     tn = confusion_matrix(testy, testy_pred)[0][0]
#     fp = confusion_matrix(testy, testy_pred)[0][1]
#     fn = confusion_matrix(testy, testy_pred)[1][0]
# 
#     print('True Positive Cases: {}'.format(tp))
#     print('True Negative Cases: {}'.format(tn))
#     print('False Positive Cases: {}'.format(fp))
#     print('False Negative Cases: {}'.format(fn))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# knn_cython(trainx, trainy, testx, testy)

"""---
#### ◉ Random Forest (Python)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Define the Random Forest model
# model = RandomForestClassifier(random_state=42)
# 
# # Define the parameter grid for Grid Search
# param_grid = {
#     'n_estimators': [200, 300],
#     'max_depth': [None, 10, 20, 40],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 6]
# }
# 
# # Create Grid Search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
# # Fit the model to find the best parameters
# grid_search.fit(trainx, trainy)
# 
# # Get the best parameters
# best_params = grid_search.best_params_
# print("Best Parameters:", best_params)
# 
# # Use the best parameters to create the final model
# best_model = RandomForestClassifier(random_state=42, **best_params)
# best_model.fit(trainx, trainy)
# 
# # Make predictions on the training set
# trainy_pred = best_model.predict(trainx)
# 
# # Evaluate the model on the training set
# accuracy_train = accuracy_score(trainy, trainy_pred)
# print("Training Accuracy:", accuracy_train)
# 
# # Make predictions on the test set
# testy_pred = best_model.predict(testx)
# 
# # Evaluate the model on the test set
# accuracy = accuracy_score(testy, testy_pred)
# report = classification_report(testy, testy_pred, digits=5)
# auc = roc_auc_score(testy, testy_pred)
# f2 = fbeta_score(testy, testy_pred, beta=2)
# precision_per_class = precision_score(testy, testy_pred, average=None)
# f1 = fbeta_score(testy, testy_pred, beta=1)
# average_precision = average_precision_score(testy, testy_pred)
# weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
# # Additional Metrics
# print("Accuracy:", accuracy)
# print("AUC:", auc)
# print("F1-score:", f1)
# print("F2-score:", f2)
# print("Precision per class:", precision_per_class)
# print("Average Precision:", average_precision)
# print("Weighted Average Recall:", weighted_recall)
# print("Classification Report:\n", report)
# 
# # Confusion Matrix Visualization
# plt.rcParams['figure.figsize'] = (4, 4)
# sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
# plt.title('Confusion Matrix')
# plt.show()
# 
# # True Positive, True Negative, False Positive, False Negative
# tp = confusion_matrix(testy, testy_pred)[1][1]
# tn = confusion_matrix(testy, testy_pred)[0][0]
# fp = confusion_matrix(testy, testy_pred)[0][1]
# fn = confusion_matrix(testy, testy_pred)[1][0]
# 
# print('True Positive Cases: {}'.format(tp))
# print('True Negative Cases: {}'.format(tn))
# print('False Positive Cases: {}'.format(fp))
# print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ Random Forest (Cython + Parallelized GridSearchCV Process)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%cython -a
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, fbeta_score, precision_score, recall_score, average_precision_score, confusion_matrix
# import matplotlib.pyplot as plt
# from joblib import parallel_backend
# import seaborn as sns
# cimport numpy as np
# cimport cython
# 
# @cython.boundscheck(False)
# @cython.wraparound(False)
# 
# def random_forest_cython(np.ndarray[np.float64_t, ndim=2] trainx, np.ndarray[np.int64_t, ndim=1] trainy, np.ndarray[np.float64_t, ndim=2] testx, np.ndarray[np.int64_t, ndim=1] testy):
#     # Define the Random Forest model
#     model = RandomForestClassifier(random_state=42)
# 
#     # Define the parameter grid for Grid Search
#     param_grid = {
#         'n_estimators': [200, 300],
#         'max_depth': [None, 10, 20, 40],
#         'min_samples_split': [2, 5, 10],
#         'min_samples_leaf': [1, 2, 6]
#     }
# 
#     with parallel_backend('multiprocessing', n_jobs=-1):
#         # Create Grid Search with cross-validation
#         grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
#         # Fit the model to find the best parameters
#         grid_search.fit(trainx, trainy)
# 
#     # Get the best parameters
#     best_params = grid_search.best_params_
#     print("Best Parameters:", best_params)
# 
#     # Use the best parameters to create the final model
#     best_model = RandomForestClassifier(random_state=42, **best_params)
#     best_model.fit(trainx, trainy)
# 
#     # Make predictions on the training set
#     trainy_pred = best_model.predict(trainx)
# 
#     # Evaluate the model on the training set
#     accuracy_train = accuracy_score(trainy, trainy_pred)
#     print("Training Accuracy:", accuracy_train)
# 
#     # Make predictions on the test set
#     testy_pred = best_model.predict(testx)
# 
#     # Evaluate the model on the test set
#     accuracy = accuracy_score(testy, testy_pred)
#     report = classification_report(testy, testy_pred, digits=5)
#     auc = roc_auc_score(testy, testy_pred)
#     f2 = fbeta_score(testy, testy_pred, beta=2)
#     precision_per_class = precision_score(testy, testy_pred, average=None)
#     f1 = fbeta_score(testy, testy_pred, beta=1)
#     average_precision = average_precision_score(testy, testy_pred)
#     weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
#     # Additional Metrics
#     print("Accuracy:", accuracy)
#     print("AUC:", auc)
#     print("F1-score:", f1)
#     print("F2-score:", f2)
#     print("Precision per class:", precision_per_class)
#     print("Average Precision:", average_precision)
#     print("Weighted Average Recall:", weighted_recall)
#     print("Classification Report:\n", report)
# 
#     # Confusion Matrix Visualization
#     plt.rcParams['figure.figsize'] = (4, 4)
#     sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
#     plt.title('Confusion Matrix')
#     plt.show()
# 
#     # True Positive, True Negative, False Positive, False Negative
#     tp = confusion_matrix(testy, testy_pred)[1][1]
#     tn = confusion_matrix(testy, testy_pred)[0][0]
#     fp = confusion_matrix(testy, testy_pred)[0][1]
#     fn = confusion_matrix(testy, testy_pred)[1][0]
# 
#     print('True Positive Cases: {}'.format(tp))
#     print('True Negative Cases: {}'.format(tn))
#     print('False Positive Cases: {}'.format(fp))
#     print('False Negative Cases: {}'.format(fn))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# random_forest_cython(trainx, trainy, testx, testy)

"""---
#### ◉ XGBoost (Python)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Define the XGBoost model
# model = XGBClassifier(random_state=42)
# 
# # Define the parameter grid for Grid Search
# param_grid = {
#     'n_estimators': [100, 200],
#     'max_depth': [3, 7, 10],  # Adjust the maximum depth as needed
#     'learning_rate': [0.01, 0.1],
#     'subsample': [0.8, 1.0],
#     'colsample_bytree': [0.8, 1.0],
#     'reg_alpha': [0, 1e-1, 1, 10],
#     'reg_lambda': [0, 1e-2, 1, 10]
# }
# 
# # Create Grid Search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
# # Fit the model to find the best parameters
# grid_search.fit(trainx, trainy)
# 
# # Get the best parameters
# best_params = grid_search.best_params_
# print("Best Parameters:", best_params)
# 
# # Use the best parameters to create the final model
# best_model = XGBClassifier(random_state=42, **best_params)
# best_model.fit(trainx, trainy)
# 
# # Make predictions on the training set
# trainy_pred = best_model.predict(trainx)
# 
# # Evaluate the model on the training set
# accuracy_train = accuracy_score(trainy, trainy_pred)
# print("Training Accuracy:", accuracy_train)
# 
# # Make predictions on the test set
# testy_pred = best_model.predict(testx)
# 
# # Evaluate the model on the test set
# accuracy = accuracy_score(testy, testy_pred)
# report = classification_report(testy, testy_pred, digits=5)
# auc = roc_auc_score(testy, testy_pred)
# f2 = fbeta_score(testy, testy_pred, beta=2)
# precision_per_class = precision_score(testy, testy_pred, average=None)
# f1 = fbeta_score(testy, testy_pred, beta=1)
# average_precision = average_precision_score(testy, testy_pred)
# weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
# # Additional Metrics
# print("Accuracy:", accuracy)
# print("AUC:", auc)
# print("F1-score:", f1)
# print("F2-score:", f2)
# print("Precision per class:", precision_per_class)
# print("Average Precision:", average_precision)
# print("Weighted Average Recall:", weighted_recall)
# print("Classification Report:\n", report)
# 
# # Confusion Matrix Visualization
# plt.rcParams['figure.figsize'] = (4, 4)
# sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
# plt.title('Confusion Matrix')
# plt.show()
# 
# # True Positive, True Negative, False Positive, False Negative
# tp = confusion_matrix(testy, testy_pred)[1][1]
# tn = confusion_matrix(testy, testy_pred)[0][0]
# fp = confusion_matrix(testy, testy_pred)[0][1]
# fn = confusion_matrix(testy, testy_pred)[1][0]
# 
# print('True Positive Cases: {}'.format(tp))
# print('True Negative Cases: {}'.format(tn))
# print('False Positive Cases: {}'.format(fp))
# print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ XGBoost (Cython + Parallelized GridSearchCV Process)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%cython -a
# from xgboost import XGBClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, fbeta_score, precision_score, recall_score, average_precision_score, confusion_matrix
# import matplotlib.pyplot as plt
# from joblib import parallel_backend
# import seaborn as sns
# cimport numpy as np
# cimport cython
# 
# @cython.boundscheck(False)
# @cython.wraparound(False)
# 
# def xgboost_cython(np.ndarray[np.float64_t, ndim=2] trainx, np.ndarray[np.int64_t, ndim=1] trainy, np.ndarray[np.float64_t, ndim=2] testx, np.ndarray[np.int64_t, ndim=1] testy):
#     # Define the XGBoost model
#     model = XGBClassifier(random_state=42)
# 
#     # Define the parameter grid for Grid Search
#     param_grid = {
#         'n_estimators': [100, 200],
#         'max_depth': [3, 7, 10],  # Adjust the maximum depth as needed
#         'learning_rate': [0.01, 0.1],
#         'subsample': [0.8, 1.0],
#         'colsample_bytree': [0.8, 1.0],
#         'reg_alpha': [0, 1e-1, 1, 10],
#         'reg_lambda': [0, 1e-2, 1, 10]
#     }
# 
#     with parallel_backend('multiprocessing', n_jobs=-1):
#         # Create Grid Search with cross-validation
#         grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
#         # Fit the model to find the best parameters
#         grid_search.fit(trainx, trainy)
# 
#     # Get the best parameters
#     best_params = grid_search.best_params_
#     print("Best Parameters:", best_params)
# 
#     # Use the best parameters to create the final model
#     best_model = XGBClassifier(random_state=42, **best_params)
#     best_model.fit(trainx, trainy)
# 
#     # Make predictions on the training set
#     trainy_pred = best_model.predict(trainx)
# 
#     # Evaluate the model on the training set
#     accuracy_train = accuracy_score(trainy, trainy_pred)
#     print("Training Accuracy:", accuracy_train)
# 
#     # Make predictions on the test set
#     testy_pred = best_model.predict(testx)
# 
#     # Evaluate the model on the test set
#     accuracy = accuracy_score(testy, testy_pred)
#     report = classification_report(testy, testy_pred, digits=5)
#     auc = roc_auc_score(testy, testy_pred)
#     f2 = fbeta_score(testy, testy_pred, beta=2)
#     precision_per_class = precision_score(testy, testy_pred, average=None)
#     f1 = fbeta_score(testy, testy_pred, beta=1)
#     average_precision = average_precision_score(testy, testy_pred)
#     weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
#     # Additional Metrics
#     print("Accuracy:", accuracy)
#     print("AUC:", auc)
#     print("F1-score:", f1)
#     print("F2-score:", f2)
#     print("Precision per class:", precision_per_class)
#     print("Average Precision:", average_precision)
#     print("Weighted Average Recall:", weighted_recall)
#     print("Classification Report:\n", report)
# 
#     # Confusion Matrix Visualization
#     plt.rcParams['figure.figsize'] = (4, 4)
#     sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
#     plt.title('Confusion Matrix')
#     plt.show()
# 
#     # True Positive, True Negative, False Positive, False Negative
#     tp = confusion_matrix(testy, testy_pred)[1][1]
#     tn = confusion_matrix(testy, testy_pred)[0][0]
#     fp = confusion_matrix(testy, testy_pred)[0][1]
#     fn = confusion_matrix(testy, testy_pred)[1][0]
# 
#     print('True Positive Cases: {}'.format(tp))
#     print('True Negative Cases: {}'.format(tn))
#     print('False Positive Cases: {}'.format(fp))
#     print('False Negative Cases: {}'.format(fn))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xgboost_cython(trainx, trainy, testx, testy)

"""---
#### ◉ LGBM (Python)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Define the LightGBM model
# model = LGBMClassifier(random_state=42)
# 
# # Define the parameter grid for Grid Search
# param_grid = {
#     'n_estimators': [100, 200],
#     'max_depth': [3, 7, -1],  # -1 means no limit
#     'learning_rate': [0.01, 0.1],
#     'subsample': [0.8, 1.0],
#     'colsample_bytree': [0.8, 1.0],
#     'reg_alpha': [0, 1e-1, 1, 10],
#     'reg_lambda': [0, 1e-1, 1, 10]
# }
# 
# # Create Grid Search with cross-validation
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
# # Fit the model to find the best parameters
# grid_search.fit(trainx, trainy)
# 
# # Get the best parameters
# best_params = grid_search.best_params_
# print("Best Parameters:", best_params)
# 
# # Use the best parameters to create the final model
# best_model = LGBMClassifier(random_state=42, **best_params)
# best_model.fit(trainx, trainy)
# 
# # Make predictions on the training set
# trainy_pred = best_model.predict(trainx)
# 
# # Evaluate the model on the training set
# accuracy_train = accuracy_score(trainy, trainy_pred)
# print("Training Accuracy:", accuracy_train)
# 
# # Make predictions on the test set
# testy_pred = best_model.predict(testx)
# 
# # Evaluate the model on the test set
# accuracy = accuracy_score(testy, testy_pred)
# report = classification_report(testy, testy_pred, digits=5)
# auc = roc_auc_score(testy, testy_pred)
# f2 = fbeta_score(testy, testy_pred, beta=2)
# precision_per_class = precision_score(testy, testy_pred, average=None)
# f1 = fbeta_score(testy, testy_pred, beta=1)
# average_precision = average_precision_score(testy, testy_pred)
# weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
# # Additional Metrics
# print("Accuracy:", accuracy)
# print("AUC:", auc)
# print("F1-score:", f1)
# print("F2-score:", f2)
# print("Precision per class:", precision_per_class)
# print("Average Precision:", average_precision)
# print("Weighted Average Recall:", weighted_recall)
# print("Classification Report:\n", report)
# 
# # Confusion Matrix Visualization
# plt.rcParams['figure.figsize'] = (4, 4)
# sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
# plt.title('Confusion Matrix')
# plt.show()
# 
# # True Positive, True Negative, False Positive, False Negative
# tp = confusion_matrix(testy, testy_pred)[1][1]
# tn = confusion_matrix(testy, testy_pred)[0][0]
# fp = confusion_matrix(testy, testy_pred)[0][1]
# fn = confusion_matrix(testy, testy_pred)[1][0]
# 
# print('True Positive Cases: {}'.format(tp))
# print('True Negative Cases: {}'.format(tn))
# print('False Positive Cases: {}'.format(fp))
# print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ LGBM (Cython + Parallelized GridSearchCV Process)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%cython -a
# from lightgbm import LGBMClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, fbeta_score, precision_score, recall_score, average_precision_score, confusion_matrix
# import matplotlib.pyplot as plt
# from joblib import parallel_backend
# import seaborn as sns
# cimport numpy as np
# cimport cython
# 
# @cython.boundscheck(False)
# @cython.wraparound(False)
# 
# def lightgbm_cython(np.ndarray[np.float64_t, ndim=2] trainx, np.ndarray[np.int64_t, ndim=1] trainy, np.ndarray[np.float64_t, ndim=2] testx, np.ndarray[np.int64_t, ndim=1] testy):
#     # Define the LightGBM model
#     model = LGBMClassifier(random_state=42)
# 
#     # Define the parameter grid for Grid Search
#     param_grid = {
#         'n_estimators': [100, 200],
#         'max_depth': [3, 7, -1],  # -1 means no limit
#         'learning_rate': [0.01, 0.1],
#         'subsample': [0.8, 1.0],
#         'colsample_bytree': [0.8, 1.0],
#         'reg_alpha': [0, 1e-1, 1, 10],
#         'reg_lambda': [0, 1e-1, 1, 10]
#     }
# 
#     with parallel_backend('multiprocessing', n_jobs=-1):
#         # Create Grid Search with cross-validation
#         grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
# 
#         # Fit the model to find the best parameters
#         grid_search.fit(trainx, trainy)
# 
#     # Get the best parameters
#     best_params = grid_search.best_params_
#     print("Best Parameters:", best_params)
# 
#     # Use the best parameters to create the final model
#     best_model = LGBMClassifier(random_state=42, **best_params)
#     best_model.fit(trainx, trainy)
# 
#     # Make predictions on the training set
#     trainy_pred = best_model.predict(trainx)
# 
#     # Evaluate the model on the training set
#     accuracy_train = accuracy_score(trainy, trainy_pred)
#     print("Training Accuracy:", accuracy_train)
# 
#     # Make predictions on the test set
#     testy_pred = best_model.predict(testx)
# 
#     # Evaluate the model on the test set
#     accuracy = accuracy_score(testy, testy_pred)
#     report = classification_report(testy, testy_pred, digits=5)
#     auc = roc_auc_score(testy, testy_pred)
#     f2 = fbeta_score(testy, testy_pred, beta=2)
#     precision_per_class = precision_score(testy, testy_pred, average=None)
#     f1 = fbeta_score(testy, testy_pred, beta=1)
#     average_precision = average_precision_score(testy, testy_pred)
#     weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
#     # Additional Metrics
#     print("Accuracy:", accuracy)
#     print("AUC:", auc)
#     print("F1-score:", f1)
#     print("F2-score:", f2)
#     print("Precision per class:", precision_per_class)
#     print("Average Precision:", average_precision)
#     print("Weighted Average Recall:", weighted_recall)
#     print("Classification Report:\n", report)
# 
#     # Confusion Matrix Visualization
#     plt.rcParams['figure.figsize'] = (4, 4)
#     sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
#     plt.title('Confusion Matrix')
#     plt.show()
# 
#     # True Positive, True Negative, False Positive, False Negative
#     tp = confusion_matrix(testy, testy_pred)[1][1]
#     tn = confusion_matrix(testy, testy_pred)[0][0]
#     fp = confusion_matrix(testy, testy_pred)[0][1]
#     fn = confusion_matrix(testy, testy_pred)[1][0]
# 
#     print('True Positive Cases: {}'.format(tp))
#     print('True Negative Cases: {}'.format(tn))
#     print('False Positive Cases: {}'.format(fp))
#     print('False Negative Cases: {}'.format(fn))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xgboost_cython(trainx, trainy, testx, testy)

"""---
#### ◉ Ensemble (LGBM + RF + XGBoost)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Define individual models with parameters
# lgbm_params = {
#     'random_state': 42,
#     'n_estimators': 200,
#     'max_depth': 3,
#     'learning_rate': 0.1,
#     'subsample': 0.8,
#     'colsample_bytree': 1.0,
#     'reg_alpha': 10,
#     'reg_lambda': 1
# }
# lgbm_model = LGBMClassifier(**lgbm_params)
# 
# rf_params = {
#     'random_state': 42,
#     'n_estimators': 350,
#     'max_depth': 30,
#     'min_samples_split': 5,
#     'min_samples_leaf': 4
# }
# rf_model = RandomForestClassifier(**rf_params)
# 
# xgb_params = {
#     'random_state': 42,
#     'n_estimators': 200,
#     'max_depth': 3,
#     'learning_rate': 0.1,
#     'subsample': 0.8,
#     'colsample_bytree': 1.0,
#     'reg_alpha': 10,
#     'reg_lambda': 1
# }
# xgb_model = XGBClassifier(**xgb_params)
# 
# # Create a voting classifier
# voting_model = VotingClassifier(
#     estimators=[
#         ('lgbm', lgbm_model),
#         ('rf', rf_model),
#         ('xgb', xgb_model)
#     ],
#     voting='soft'  # Use 'soft' for probability voting
# )
# 
# # Fit the ensemble model to the training data
# voting_model.fit(trainx, trainy)
# 
# # Make predictions on the training set
# trainy_pred = voting_model.predict(trainx)
# 
# # Evaluate the model on the training set
# accuracy_train = accuracy_score(trainy, trainy_pred)
# print("Training Accuracy:", accuracy_train)
# 
# # Make predictions on the test set
# testy_pred = voting_model.predict(testx)
# 
# # Evaluate the model on the test set
# accuracy = accuracy_score(testy, testy_pred)
# report = classification_report(testy, testy_pred, digits=5)
# auc = roc_auc_score(testy, testy_pred)
# f2 = fbeta_score(testy, testy_pred, beta=2)
# precision_per_class = precision_score(testy, testy_pred, average=None)
# f1 = fbeta_score(testy, testy_pred, beta=1)
# average_precision = average_precision_score(testy, testy_pred)
# weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
# # Additional Metrics
# print("Accuracy:", accuracy)
# print("AUC:", auc)
# print("F1-score:", f1)
# print("F2-score:", f2)
# print("Precision per class:", precision_per_class)
# print("Average Precision:", average_precision)
# print("Weighted Average Recall:", weighted_recall)
# print("Classification Report:\n", report)
# 
# # Confusion Matrix Visualization
# plt.rcParams['figure.figsize'] = (4, 4)
# sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
# plt.title('Confusion Matrix')
# plt.show()
# 
# # True Positive, True Negative, False Positive, False Negative
# tp = confusion_matrix(testy, testy_pred)[1][1]
# tn = confusion_matrix(testy, testy_pred)[0][0]
# fp = confusion_matrix(testy, testy_pred)[0][1]
# fn = confusion_matrix(testy, testy_pred)[1][0]
# 
# print('True Positive Cases: {}'.format(tp))
# print('True Negative Cases: {}'.format(tn))
# print('False Positive Cases: {}'.format(fp))
# print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ TabNet (Python)
"""

from pytorch_tabnet.tab_model import TabNetClassifier

# Define TabNet parameters
tabnet_params = {
    # TabNet parameters
}

# Create a TabNet model
tabnet_model = TabNetClassifier(**tabnet_params)

# Fit the TabNet model to the training data
tabnet_model.fit(trainx, trainy)

# Make predictions on the training set
trainy_pred = tabnet_model.predict(trainx)

# Evaluate the model on the training set
accuracy_train = accuracy_score(trainy, trainy_pred)
print("Training Accuracy:", accuracy_train)

# Make predictions on the test set
testy_pred = tabnet_model.predict(testx)

# Evaluate the model on the test set
accuracy = accuracy_score(testy, testy_pred)
report = classification_report(testy, testy_pred, digits=5)
auc = roc_auc_score(testy, testy_pred)
f2 = fbeta_score(testy, testy_pred, beta=2)
precision_per_class = precision_score(testy, testy_pred, average=None)
f1 = fbeta_score(testy, testy_pred, beta=1)
average_precision = average_precision_score(testy, testy_pred)
weighted_recall = recall_score(testy, testy_pred, average='weighted')

# Additional Metrics
print("Accuracy:", accuracy)
print("AUC:", auc)
print("F1-score:", f1)
print("F2-score:", f2)
print("Precision per class:", precision_per_class)
print("Average Precision:", average_precision)
print("Weighted Average Recall:", weighted_recall)
print("Classification Report:\n", report)

# Confusion Matrix Visualization
plt.rcParams['figure.figsize'] = (4, 4)
sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
plt.title('Confusion Matrix')
plt.show()

# True Positive, True Negative, False Positive, False Negative
tp = confusion_matrix(testy, testy_pred)[1][1]
tn = confusion_matrix(testy, testy_pred)[0][0]
fp = confusion_matrix(testy, testy_pred)[0][1]
fn = confusion_matrix(testy, testy_pred)[1][0]

print('True Positive Cases: {}'.format(tp))
print('True Negative Cases: {}'.format(tn))
print('False Positive Cases: {}'.format(fp))
print('False Negative Cases: {}'.format(fn))

"""---
#### ◉ Optimized Neural Network (Python)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# class ONN(nn.Module):
#     def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.25):
#         super(ONN, self).__init__()
# 
#         layers = []
#         layers.append(nn.Linear(input_size, hidden_sizes[0]))
#         layers.append(nn.PReLU())  # Using PReLU as an advanced activation function
#         layers.append(nn.BatchNorm1d(hidden_sizes[0]))
#         layers.append(nn.Dropout(p=dropout_rate))
# 
#         for i in range(len(hidden_sizes) - 1):
#             layers.append(self._make_layer(hidden_sizes[i], hidden_sizes[i+1], dropout_rate))
# 
#         self.hidden_layers = nn.Sequential(*layers)
#         self.output_layer = nn.Linear(hidden_sizes[-1], output_size)
#         self._initialize_weights()
#         self.threshold = 0.5
# 
#     def _make_layer(self, in_features, out_features, dropout_rate):
#         layer = nn.Sequential(
#             nn.Linear(in_features, out_features),
#             nn.PReLU(),
#             nn.BatchNorm1d(out_features),
#             nn.Dropout(p=dropout_rate)
#         )
#         return layer
# 
#     def _initialize_weights(self):
#         for m in self.modules():
#             if isinstance(m, nn.Linear):
#                 nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He initialization
#                 if m.bias is not None:
#                     nn.init.constant_(m.bias, 0)
# 
#     def set_threshold(self, threshold):
#         self.threshold = threshold
# 
#     def forward(self, x):
#         x = self.hidden_layers(x)
#         x = self.output_layer(x)
#         return x
# 
#     def predict(self, x):
#         with torch.no_grad():
#             self.eval()
#             y_prob = F.softmax(self.forward(x), dim=1)  # Ensure to use self.forward
#             y_pred = (y_prob[:, 1] > self.threshold).int().numpy()
#         return y_pred
# 
# # Set the input, hidden, and output sizes
# input_size = 12  # The number of features
# hidden_sizes = [64, 128]
# output_size = 2
# 
# # Create the neural network model
# neural_net_model = ONN(input_size, hidden_sizes, output_size)
# 
# # Define loss function and optimizer
# criterion = nn.CrossEntropyLoss()
# optimizer = torch.optim.AdamW(neural_net_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
# 
# # Convert data to PyTorch tensors if not already
# trainx_tensor = torch.Tensor(trainx)
# trainy_tensor = torch.LongTensor(trainy)  # Assuming classification with integers
# 
# # Training the neural network
# num_epochs = 1200
# train_losses = []
# 
# for epoch in range(num_epochs):
#     # Forward pass
#     outputs = neural_net_model(trainx_tensor)
#     loss = criterion(outputs, trainy_tensor)
#     train_losses.append(loss.item())
# 
#     # Backward and optimize
#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()
# 
# # Plot the training loss over epochs
# plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Training Loss Over Epochs')
# plt.legend()
# plt.show()
# 
# # Evaluation on the training set
# with torch.no_grad():
#     neural_net_model.eval()
#     trainy_pred = neural_net_model.predict(trainx_tensor)
#     accuracy_train = accuracy_score(trainy, trainy_pred)
#     print("Training Accuracy:", accuracy_train)
# 
# # Evaluation on the test set
# testx_tensor = torch.Tensor(testx)
# testy_tensor = torch.LongTensor(testy)
# with torch.no_grad():
#     neural_net_model.eval()
#     neural_net_model.set_threshold(0.45)  # Adjust the threshold
#     testy_pred = neural_net_model.predict(testx_tensor)
#     # Evaluate the model on the test set
#     accuracy = accuracy_score(testy, testy_pred)
#     report = classification_report(testy, testy_pred, digits=5)
#     auc = roc_auc_score(testy, testy_pred)
#     f2 = fbeta_score(testy, testy_pred, beta=2)
#     precision_per_class = precision_score(testy, testy_pred, average=None)
#     f1 = fbeta_score(testy, testy_pred, beta=1)
#     average_precision = average_precision_score(testy, testy_pred)
#     weighted_recall = recall_score(testy, testy_pred, average='weighted')
# 
#     # Additional Metrics
#     print("Accuracy:", accuracy)
#     print("AUC:", auc)
#     print("F1-score:", f1)
#     print("F2-score:", f2)
#     print("Precision per class:", precision_per_class)
#     print("Average Precision:", average_precision)
#     print("Weighted Average Recall:", weighted_recall)
#     print("Classification Report:\n", report)
# 
#     # Confusion Matrix Visualization
#     plt.rcParams['figure.figsize'] = (4, 4)
#     sns.heatmap(confusion_matrix(testy, testy_pred), annot=True, fmt='d', linewidths=.5, cmap="YlGnBu")
#     plt.title('Confusion Matrix')
#     plt.show()
# 
#     # True Positive, True Negative, False Positive, False Negative
#     tp = confusion_matrix(testy, testy_pred)[1][1]
#     tn = confusion_matrix(testy, testy_pred)[0][0]
#     fp = confusion_matrix(testy, testy_pred)[0][1]
#     fn = confusion_matrix(testy, testy_pred)[1][0]
# 
#     print('True Positive Cases: {}'.format(tp))
#     print('True Negative Cases: {}'.format(tn))
#     print('False Positive Cases: {}'.format(fp))
#     print('False Negative Cases: {}'.format(fn))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.base import BaseEstimator

class PyTorchGridSearchWrapper(BaseEstimator):
    def __init__(self, input_size, num_epochs=1200, hidden_sizes=(64, 128), dropout_rate=0.25, threshold=0.45):
        self.input_size = input_size
        self.num_epochs = num_epochs
        self.hidden_sizes = hidden_sizes
        self.dropout_rate = dropout_rate
        self.threshold = threshold
        self.model = None

    # Setup the PyTorch model
    def _init_model(self):
        self.model = ONN(self.input_size, self.hidden_sizes, output_size, self.dropout_rate)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)

    # Fit method for GridSearchCV
    def fit(self, X, y):
        X_tensor = torch.Tensor(X)
        y_tensor = torch.LongTensor(y)
        self._init_model()

        for epoch in range(self.num_epochs):
            # Forward pass
            outputs = self.model(X_tensor)
            loss = self.criterion(outputs, y_tensor)

            # Backward and optimize
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
        return self

    # Predict method for GridSearchCV
    def predict(self, X):
        self.model.set_threshold(self.threshold)
        X_tensor = torch.Tensor(X)
        return self.model.predict(X_tensor)

# Custom scoring function that sklearn's GridSearchCV can use
def custom_score(estimator, X_test, y_true):
    y_pred = estimator.predict(X_test)
    return accuracy_score(y_true, y_pred)

# Parameter Grid to Search
param_grid = {
    'num_epochs': [1000, 1100, 1200],
    'hidden_sizes': [(32, 32), (32, 64), (64, 64), (64, 128)],
    'dropout_rate': [0.1, 0.25, 0.5],
    'threshold': [0.45, 0.5]
}

input_size = trainx.shape[1]
output_size = len(set(trainy))

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=PyTorchGridSearchWrapper(input_size=input_size),
                           param_grid=param_grid,
                           scoring=custom_score,
                           cv=5)

# Execute the grid search
grid_search.fit(trainx, trainy)

# The best hyperparameters
print(grid_search.best_params_)
best_model = grid_search.best_estimator_.model

